---
title: "v181 - Library Access"
author: "how"
format: html
editor: visual
---

## Description:

Library visits per person living within the library service area per year.

## Data Source:

Institute of Museum and Library Services

Data Download Link: https://www.imls.gov/research-evaluation/surveys/public-libraries-survey-pls

FY2022 data was used for 2025 Rankings

FY2023 data was used for 2026 Rankings

## Measure Calculation:

### National

National values are calculated like state values. The sum of library visits across the country and the national population are calculated, then the two numbers are divided.

### State-level

The state value is calculated by summing the library visits and the population from that state, then dividing the two sums.

### County-level

Since a library's service area may cross multiple counties, we calculate the median number of visits for all libraries with service areas in the county and use this as the county value.

1.  Bring in and clean library data files

Library data is provided in an outlet file (pls_fy22_outlet_pud22i) and an AE file (PLS_FY22_AE_pud22i).

The following code is provided by the data source:

Outlet file:

array num *numeric*; do over num; \* \* if num = -1 then num = .M; /*recode missing value into .M*/ if num = -3 and STATSTRU ='23' then num = .C; /*recode Temporary Closed Library into .C*/ if num = -4 then num = .N; /*recode "Not Applicable" into .N*/ end; array char *character*; do over char; if char ='M' then char = ' '; /*recode missing value into M for character variables*/ end; /*recode the rest of special missing into corresponding missing values*/

Apply the following criteria to clean the files:

Outlet file:

if STATSTRU in ('23') then delete; \***if they were temporarily closed during this year**; if phone in ('-3') then delete; if hours =-3 then delete; if wks_open \< 2 then delete; **if they were open less than 2 weeks**;

IF STATSTRU = '10' THEN DELETE; \*this denotes an incorrect record in the data set;

if C_FSCS = "N" then delete; ***not a public library***; if C_OUT_TY = "BM" then delete; ***if the outlet was a book by mail only***; if STABR = "AS" or STABR = "GU" or STABR = "MP" or STABR = "PR" or STABR = "VI" then delete; **american samoa, guam, mariana islands, PR, virgin islands**\*;

AE file:

if STARTDAT = '-3' then STARTDAT = ' '; if ENDDATE = '-3' then ENDDATE = ' '; IF C_LEGBAS = 'SD' THEN DELETE; \*delete school district libraries that wouldn’t be for the general public;

if C_FSCS = "N" then delete; **not a public library**; if STABR = "AS" or STABR = "GU" or STABR = "MP" or STABR = "PR" or STABR = "VI" then delete; **american samoa, guam, mariana islands, PR, virgin islands***; if STATSTRU in ('-23') then delete;* **if they were temporarily closed during this year**; if visits IN (-3, -1, 0) then delete; if phone in ('-3') then delete; if HRS_open =-3 then delete;

NOTE: take a substring of CENTRACT to get state (SUBSTR(CENTRACT,1,2); and county (SUBSTR(CENTRACT,3,3) fips, but ensure you label the resulting variables to differentiate between the fips originating from the outlet file vs. the AE file to avoid overwriting during merge.

2.  Calculate visits per population from AE file

visits_per_pop=visits/popu_und

3.  Merge the Outlet and AE file by FSCSKEY

Delete any rows where variable hours is missing--if hours =. then delete;

4.  Perform PROC UNIVARIATE (or similar) to calculate the median value of visits_per_pop and CIs (**SEE NOTES ABOUT THIS STEP**)

proc univariate DATA=\[have\] cipctldf alpha=0.05 noprint; var \[variable name for visits/population\]; by statecode_outlet countycode_outlet; \*use the fips from the outlet file; OUTPUT OUT=visit_median_ci pctlpts=50 pctlpre=p cipctldf=(lowerpre=LCL upperpre=UCL); run;

Any rows where cilow=cihigh should have their cilow and cihigh set to missing

5.  Merge with county dataset so any missing counties can be included

6.  Calculate state values from AE file

7.  Calculate national values from AE file

8.  Calculate state and national CIs

Similar to how we generally calculate CIs.

9.  Merge and clean up final dataset to follow CHRR formatting of measure datasets

# import packages

```{r}
library(tidyverse)
library(haven)
library(here)

```

# load standard county and state fips

The final dataset will need to be aligned with standard FIPS codes and to include all counties and states.

```{r}
fips <-  bind_rows(
  read_sas(here("inputs/county_fips_with_ct_old.sas7bdat")) ,
  read_sas(here("inputs/state_fips.sas7bdat")))  %>% 
  arrange(statecode, countycode) %>% 
  glimpse()
```

# import raw data

```{r}
aeraw = read.csv(here("raw_data/IMLS/PLS_FY23_AE_pud23i.csv"))
outletraw = read.csv(here("raw_data/IMLS/pls_fy23_outlet_pud23i.csv"))

```

# clean the ae file 

```{r}

aeclean = aeraw %>% 
  #filter(!is.na(HOURS)) %>% 
  filter(!(STATSTRU %in% c(23))) %>% #c(10, 23))) %>% 
  filter(!(PHONE %in% c(-3))) %>% #, -4))) %>% 
  #filter(HOURS != -3) %>% 
  #filter(STARTDAT != -3) %>% #start and end date shoudl be chekced - unclear that they need to be deleted..... 
  #filter(ENDDATE != -3) %>% 
  filter(C_LEGBAS != "SD") %>% 
  filter(HRS_OPEN != -3) %>% 
  #filter(WKS_OPEN >= 2) %>% 
  filter(C_FSCS != "N") %>% 
  #filter(C_OUT_TY != "BM") %>% 
  filter(!(STABR %in% c("AS", "GU", "MP", "PR", "VI"))) %>% 
  filter(!(VISITS %in% c(-3, -1, 0))) %>% 
  mutate(totcode = str_pad(CENTRACT, width = 11, pad = "0", side = "left"),
    statecode = substr(totcode, 1, 2), 
         countycode = substr(totcode, 3,5),
    visits_per_pop = VISITS/POPU_UND)


```

# calculate state and national values 

The visit rate is defined as the ratio of total visits to total population served (R=numerator/denominatorR = \text{numerator}/\text{denominator}R=numerator/denominator). Confidence intervals are calculated using two approaches depending on the size of the numerator:

-   For numerators greater than 99, a normal approximation is used with standard error SE=R1/nSE = R \sqrt{1/n}SE=R1/n​.

-   For numerators 99 or fewer, exact Poisson interval limits are applied using gamma quantiles and then scaled to the rate.

This is based on this [documentation available from CDC EpiInfo](https://www.cdc.gov/epiinfo/user-guide/statcalc/poisson.html)

```{r}
library(dplyr)

# alpha = 0.95
alpha <- 0.95
alo <- (1 - alpha) / 2     # lower tail
ahi <- (1 + alpha) / 2     # upper tail

calc_ci_sas <- function(num, den) {

  if (is.na(num) | is.na(den)) {
    return(c(NA, NA, NA))
  }

  R <- num / den

  # Case 1: Large numerator
  if (num > 99) {
    SE <- R * sqrt(1 / num)
    LCL <- R - 1.96 * SE
    UCL <- R + 1.96 * SE
    return(c(R, LCL, UCL))
  }

  # Case 2: Small numerator
  if (num <= 99) {
    # Exact Garwood limits
    # Lower gamma uses shape=num, upper uses shape=num+1
    L <- qgamma(alo, num) / num
    U <- qgamma(ahi, num + 1) / num

    LCL <- R * L
    UCL <- R * U
    return(c(R, LCL, UCL))
  }

  # Fallback
  return(c(R, NA, NA))
}

# calculate state vals 
statevals <- aeclean %>%
  group_by(statecode) %>%
  summarise(
    numerator = sum(VISITS, na.rm = TRUE),
    denominator = sum(POPU_UND, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  rowwise() %>%
  mutate(
    ci = list(calc_ci_sas(numerator, denominator)),
    median = ci[[1]],
    LCL = ci[[2]],
    UCL = ci[[3]]
  ) %>%
  select(statecode, numerator, denominator, median, LCL, UCL) %>% 
  filter(statecode != "00") %>% 
  mutate(countycode = "000")

# calculate ntl val  
ntlval <- aeclean %>%
  summarise(
    numerator = sum(VISITS, na.rm = TRUE),
    denominator = sum(POPU_UND, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  rowwise() %>%
  mutate(
    ci = list(calc_ci_sas(numerator, denominator)),
    median = ci[[1]],
    LCL = ci[[2]],
    UCL = ci[[3]]
  ) %>%
  select(numerator, denominator, median, LCL, UCL) %>% 
  mutate(statecode = "00", 
         countycode = "000")

```

\

# Now clean the outlet file and merge

```{r}

outletclean = outletraw %>% 
  filter(!is.na(HOURS)) %>% 
  filter(!(STATSTRU %in% c(10, 23))) %>% 
  filter(!(PHONE %in% c(-3))) %>%  # , -4))) %>% 
  filter(HOURS != -3) %>% 
  filter(WKS_OPEN >= 2) %>% 
  filter(C_FSCS != "N") %>% 
  filter(C_OUT_TY != "BM") %>% 
  filter(!(STABR %in% c("AS", "GU", "MP", "PR", "VI"))) %>% 
  mutate(totcode = str_pad(CENTRACT, width = 11, pad = "0", side = "left"),
         statecode = substr(totcode, 1, 2), 
         countycode = substr(totcode, 3,5),
         onelib = 1) %>% 
  group_by(FSCSKEY) %>%
  mutate(nlibrary = n()) %>%
  ungroup()

outletae = merge(outletclean, aeclean, by = "FSCSKEY", all.x = TRUE)
```

# Calculate county level medians and CIs 

Note: Historically, we have calculated nonparametric confidence intervals for medians using the [CIPCTLDF option in SAS's PROC UNIVARIATE](https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/procstat/procstat_univariate_syntax01.htm#procstat.univariate.proc_cipctldf), which applies [these formulas](https://go.documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/procstat/procstat_univariate_details14.htm#procstat.univariate.clpctl). However, this function is not directly replicable outside of SAS.

Several similar functions exist in R, but none appear to produce results that exactly match the SAS output. This discrepancy may be due to [incomplete documentation for the CIPCTLDF option](https://communities.sas.com/t5/Statistical-Procedures/discrepancies-in-distribution-free-confidence-interval-for/m-p/535211#M26951).

Below I've included one approach for calculating distribution-free confidence intervals. While these calculations are "correct" from a mathematical standpoint, they do not produce the same values as the currently published CHR&R data which were calculated and verified using SAS.

```{r}
#meds = outletae %>% group_by(statecode.x, countycode.x) %>%
#  rename(statecode = statecode.x, countycode = countycode.x) %>% 
#  summarize(med_visit = median(visits_per_pop, na.rm = TRUE))


medscis = outletae %>% group_by(statecode.x, countycode.x) %>% 
  rename(statecode = statecode.x, countycode= countycode.x) %>% 
  summarise(
    median_ci = list(smedian.hilow(visits_per_pop, conf.int = 0.95)),
    .groups = "drop"
  ) %>%
  mutate(
    median = sapply(median_ci, `[`, 1),
    LCL    = sapply(median_ci, `[`, 2),
    UCL    = sapply(median_ci, `[`, 3)
  ) %>%
  # Set LCL/UCL to missing if CI collapses to a point
  mutate(
    LCL = ifelse(LCL == UCL, NA, LCL),
    UCL = ifelse(LCL == UCL, NA, UCL)
  ) %>%
  select(-median_ci)

```

# merge with fipscodes 

```{r}


cfips = fips %>% filter(countycode != "000")

oa_cfips = merge(cfips, medscis, by = c("statecode", "countycode"), all.x = TRUE) %>% filter(countycode != "000")


oatot = bind_rows(oa_cfips, statevals, ntlval)

```

# rename some columns and save measure data

```{r}
libsfinal = oatot %>% rename(v181_rawvalue = median,
                             v181_numerator = numerator, 
                             v181_denominator = denominator,
                             v181_cilow = LCL,
                             v181_cihigh = UCL)
write.csv(libsfinal, file = here("measure_datasets/v181_r2026.csv"))

```

# Compare to SAS calculated data 

There are substantial differences, with the R calculated CIs being much wider in many cases.

```{r}
mb = haven::read_sas(here("temp/v181_mb.sas7bdat"))
mbh = merge(mb, libsfinal, by = c("statecode", "countycode"))

mbh$rawdiff = mbh$v181_rawvalue.x - mbh$v181_rawvalue.y
mbh$cilowdiff = mbh$v181_cilow.x - mbh$v181_cilow.y
mbh$cihighdiff = mbh$v181_cihigh.x - mbh$v181_cihigh.y

# Missingness match flags
mbh$raw_mismatch   <- is.na(mbh$v181_rawvalue.x) != is.na(mbh$v181_rawvalue.y)

mbh$cilow_mismatch <- is.na(mbh$v181_cilow.x)      != is.na(mbh$v181_cilow.y)

mbh$cihigh_mismatch <- is.na(mbh$v181_cihigh.x)    != is.na( mbh$v181_cihigh.y)
```
